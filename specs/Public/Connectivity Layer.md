## Connectivity Layer — Overlay Networking for Leader/Followers

### Purpose

Provide reliable, low‑friction connectivity for run‑everywhere and Mutagen
between the leader and follower hosts across Linux, macOS, and Windows.

Key properties:

- One overlay per host/VM, not per task. All agent sessions reuse the same
  connectivity.
- Prefer SSH as the execution transport; Mutagen spawns its agent over SSH.

### Actors (Roles)

- **Coordinator**: The controller that creates sessions, provisions followers,
  requests connectivity credentials, and orchestrates handshakes (typically the
  `aw` client or WebUI backend acting on behalf of the user).
- **Leader**: The primary workspace host (Linux preferred) that owns
  FsSnapshots and initiates fences and run‑everywhere.
- **Followers**: Secondary hosts (Windows/macOS/Linux) that execute commands
  and validate builds/tests.

### Recommended Options

- Tailscale (default)
  - WireGuard‑based mesh with automatic NAT traversal, MagicDNS, device
    tags/ACLs, and optional Tailscale SSH.
  - Simple SSO onboarding across OS’s. Suitable for parallel tasks because a
    single daemon/TUN per host serves all sessions.
  - Userspace mode for locked‑down containers: `tailscaled
--tun=userspace-networking --socks5-server=127.0.0.1:1055` and route
    SSH/Mutagen via the SOCKS proxy.
  - Self‑hosted control plane: Headscale.

  - Ephemeral nodes for short‑lived sessions: - Use ephemeral auth keys (or `--state=mem:`) so devices auto‑remove
    shortly after going offline; they receive a fresh IP each time. - Immediate cleanup: call `tailscale logout` on teardown. - Scope access via ACL tags (e.g., `tag:session-<id>` ↔
    `tag:session-<id>` only).

- ZeroTier (good alternative)
  - L2/L3 virtual network with NAT traversal and central controller. Easy
    multi‑OS setup.
  - Use allocated overlay IPs/Magic DNS in `.agents/hosts.json`.

- Raw WireGuard (minimal)
  - Fast and simple, but manual key/IP management and NAT traversal setup. Best
    for small/static topologies or when WG is already in place.

- SSH‑only (fallback)
  - Direct SSH over public/private networks, or reverse SSH tunnels if
    followers cannot accept inbound connections. More ops overhead but universally
    available.

### Operational Guidance

- Standardize on SSH
  - Mutagen can run over SSH; run‑everywhere executes remote commands via SSH.
  - Keep follower SSH access non‑root; prefer short‑lived keys or Tailscale
    SSH.

- Host Catalog
  - Store overlay addresses and metadata in `.agents/hosts.json` (or via REST):

  ```json
  {
    "hosts": [
      {
        "name": "win-01",
        "os": "windows",
        "address": "win-01.tailnet.example",
        "tags": ["os=windows"],
        "sshUser": "builder"
      },
      {
        "name": "mac-01",
        "os": "macos",
        "address": "100.101.102.103",
        "tags": ["os=macos"],
        "sshUser": "builder"
      }
    ]
  }
  ```

- Security
  - Use overlay ACLs (Tailscale/ZeroTier) to restrict leader↔follower
    reachability.
  - Disable password auth on SSH; prefer keys/SSO; limit to non‑privileged
    users.
  - For NetBird: create Setup Keys with `ephemeral=true`, optional `reusable`,
    and `auto_groups` to isolate short‑lived peers; peers auto‑purge after
    inactivity. Use API `DELETE /api/peers/{id}` for immediate removal.

- Performance
  - Co‑locate followers when possible; validate MTU; monitor sync‑fence
    latency in CI multi‑OS smoke tests.

### NetBird Support (ephemeral peers)

Workflow:

1. Coordinator requests an ephemeral Setup Key from the Agents‑Workflow REST
   service (see REST section below).
2. Followers run `netbird up --setup-key <KEY>`.
3. Access is scoped via `auto_groups` and policies (only session peers can talk
   to each other and the leader).
4. Teardown: stop the agent; peers auto‑purge after ~10 minutes of
   inactivity, or the coordinator calls `DELETE /api/peers/{id}` via NetBird API
   for immediate removal.

Notes:

- Keys may be reusable to simplify parallel follower startup; groups isolate
  session scope.
- Ephemeral peers should not be referenced by fixed IPs; use names/groups.

### Fallback Relay Mode (no overlay available)

When the server or client reports overlays are unavailable, the coordinator can
relay messages/logs between leader and followers.

- Transport: Server‑Sent Events (SSE) for downstream (client subscribe) and
  HTTP POST for upstream (client publish).
- Channels: Namespaced by `sessionId` and `host`.
- Scope: run‑everywhere control messages, stdout/stderr, exit codes, and
  minimal Mutagen control signals (not bulk sync).

Relay behavior:

- Followers subscribe to `/relay/{sessionId}/{host}/control` and
  `/relay/{sessionId}/{host}/stdin`.
- Followers POST logs to `/relay/{sessionId}/{host}/stdout` and
  `/relay/{sessionId}/{host}/stderr`, and status to
  `/relay/{sessionId}/{host}/status`.
- Leader/client multiplexes commands to followers and aggregates outputs.

### Handshake & Sync Confirmation

Goal: Confirm follower connectivity (overlay or relay) before first
run‑everywhere, with a short timeout.

Sequence (overlay path):

1. Client → REST: `POST /connect/keys` (request ephemeral keys for preferred
   provider in priority order: netbird → tailscale).
2. Server → Client: returns available provider and session‑scoped
   credentials (e.g., NetBird setup key or Tailscale ephemeral auth key) plus
   ACL/group tags.
3. Client: distributes credentials to follower instances; followers join
   overlay.
4. Client → REST: `POST /connect/handshake` with the list of expected
   followers.
5. Followers → REST: `POST /connect/handshake/ack` upon successful join
   (overlay reachability + SSH check).
6. Server → Client: `200 OK` when all acks received or timeout reached;
   includes per‑host status.

Fallback (relay path): Same handshake but without overlay checks; followers
instead establish SSE subscriptions; acks include relay stream readiness.

### Sequence Diagram (overlay path)

```mermaid
sequenceDiagram
  participant Client as Coordinator (aw client)
  participant REST as REST Service
  participant F as Followers (per host)

  Client->>REST: POST /connect/keys { providers: [netbird,tailscale] }
  REST-->>Client: { provider, credentials }
  Client->>F: distribute credentials (netbird/tailscale up)
  Client->>REST: POST /connect/handshake { sessionId, hosts }
  F-->>REST: POST /connect/handshake/ack (hostA)
  F-->>REST: POST /connect/handshake/ack (hostB)
  REST-->>Client: 200 OK { statuses }
```

### Fallback Transport (no overlay, no public IPs)

Assumptions: No TUN devices (no Tailscale/NetBird dataplane), no inbound
connectivity to the Coordinator (no public IP), and HTTP CONNECT is not
available/useful.

Approach A (VM‑hosted SOCKS + client relay hub — default in ad‑hoc
fleets):

- Each VM runs the agents‑workflow backend which exposes a local SOCKS5 proxy
  on 127.0.0.1 (inside the VM). All software in the VM (SSH, Mutagen) uses this
  local SOCKS to reach other fleet members.
- The `aw` client maintains persistent control connections to each VM backend
  and relays per‑connection bytestreams between VMs (leader ↔ follower) over
  these client↔VM links.
- This requires zero inbound connectivity and no public IPs; only outbound from
  the client to each VM.

Mermaid sequence (VM‑local SOCKS, client relay hub):

```mermaid
sequenceDiagram
  participant Proc as VM Process (SSH/Mutagen)
  participant SOCKS as VM Local SOCKS5 (agents backend)
  participant Client as aw relay hub
  participant Other as Other VM Local SOCKS5

  Proc->>SOCKS: CONNECT follower-01:22
  SOCKS->>Client: open stream to follower-01
  Client->>Other: open stream request
  Other-->>Client: stream ready (to 127.0.0.1:22)
  Client-->>SOCKS: stream ready
  Proc-->>SOCKS: SSH bytes
  SOCKS-->>Client: forward bytes
  Client-->>Other: forward bytes
  Other-->>Client: response bytes
  Client-->>SOCKS: response bytes
  SOCKS-->>Proc: response bytes
```

Approach B (server‑hosted): A REST‑hosted, per‑session SOCKS5 rendezvous
that tunnels TCP streams over WebSockets.

- **Session SOCKS5 Relay (REST‑hosted)**
  - The REST service exposes a SOCKS5 front‑end that does not reach the
    public internet. Instead, it maps "destinations" to registered peers
    (leader/followers) connected via WebSocket.
  - Peers register their local endpoints (e.g., `ssh: 127.0.0.1:22`) over a
    persistent WebSocket: `WS
/api/v1/connect/socks/register?peerId=...&role=leader|follower`.
  - The SOCKS5 server accepts `CONNECT follower-01:22` from the leader’s SSH
    and forwards bytes over the follower’s WebSocket to its local `127.0.0.1:22`.
  - Similarly, Mutagen using SSH as transport routes through `ProxyCommand` to
    the session SOCKS5 server.

- **Why this works without public IPs**
  - Both leader and followers initiate outbound WebSocket connections to the
    REST service. The service stitches the bytestreams, acting as a rendezvous.

- **Client configuration**
  - SSH config on leader (example):

  ```
  Host follower-01
    HostName follower-01
    ProxyCommand nc -X 5 -x socks.rest.example:1080 %h %p
  ```

  - Mutagen: configure SSH to use the same `ProxyCommand`.

Mermaid sequence (server‑hosted SOCKS5 rendezvous):

```mermaid
sequenceDiagram
  participant L as Leader SSH
  participant S as REST Session SOCKS5
  participant WS as REST WS Hub
  participant F as Follower (WS agent)

  F->>WS: WS register peerId=follower-01 targets={ssh:127.0.0.1:22}
  L->>S: SOCKS5 CONNECT follower-01:22
  S->>WS: open stream to peer follower-01 (ssh)
  WS-->>S: stream ready
  S-->>L: SOCKS5 200 OK
  L-->>S: SSH bytes
  S-->>WS: forward bytes
  WS-->>F: forward to 127.0.0.1:22
  F-->>WS: SSH response bytes
  WS-->>S: forward bytes
  S-->>L: SSH response bytes
```

Note: In both approaches, HTTP CONNECT is not assumed available and public IPs
are not required.

### Userspace VPN SOCKS fallback (when TUN fails)

If the Coordinator has provided ephemeral overlay credentials
(NetBird/Tailscale), but VMs fail to create TUN interfaces, each VM launches a
userspace VPN daemon that exposes a local SOCKS5 proxy. All VM processes (SSH,
Mutagen, etc.) use this local SOCKS as above, and traffic is relayed either by
the aw client (ad‑hoc fleets) or by the REST server (server‑hosted
rendezvous).
