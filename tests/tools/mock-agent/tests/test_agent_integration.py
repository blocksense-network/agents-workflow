#!/usr/bin/env python3
"""
Integration tests for mock-agent API server with real CLI tools.

These tests verify that claude and codex CLI tools can successfully
interact with our mock-agent server to perform file editing operations
in a temporary workspace.
"""

import json
import os
import subprocess
import tempfile
import threading
import time
import unittest
from typing import Optional, Dict, Any
import signal
import shutil
import sys

try:
    import pexpect
    PEXPECT_AVAILABLE = True
except ImportError:
    PEXPECT_AVAILABLE = False

# Add the src directory to Python path for imports
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

# Import server module and make it available
import importlib.util
server_spec = importlib.util.spec_from_file_location("server", os.path.join(os.path.dirname(__file__), '..', 'src', 'server.py'))
server_module = importlib.util.module_from_spec(server_spec)
sys.modules["server"] = server_module
server_spec.loader.exec_module(server_module)
serve = server_module.serve


class MockAgentIntegrationTest(unittest.TestCase):
    """Test class for mock-agent integration with CLI tools."""
    
    @classmethod
    def setUpClass(cls):
        """Set up shared test resources."""
        cls.test_dir = tempfile.mkdtemp(prefix="mock_agent_test_")
        cls.server_port = 18080  # Use a high port to avoid conflicts
        cls.server_host = "127.0.0.1"
        cls.server_process = None
        cls.server_thread = None

        # Create test playbooks and scenarios
        cls.setup_test_files()

        # Set up hooks for real agents
        cls.setup_agent_hooks()

        # Start the mock server
        cls.start_mock_server()

        # Wait for server to be ready
        time.sleep(2)
    
    @classmethod
    def tearDownClass(cls):
        """Clean up test resources."""
        if cls.server_process:
            cls.server_process.terminate()
            cls.server_process.wait()
        
        if cls.server_thread:
            # Server should stop when main thread ends
            cls.server_thread.join(timeout=5)
        
        # Clean up test directory
        shutil.rmtree(cls.test_dir, ignore_errors=True)

    @classmethod
    def setup_agent_hooks(cls):
        """Set up hooks for Claude Code and Codex agents."""
        hook_script_path = os.path.join(os.path.dirname(__file__), "..", "hooks", "simulate_snapshot.py")

        # Set up Claude Code hooks in a temporary directory
        cls.claude_fake_home = os.path.join(cls.test_dir, "fake_claude_home")
        os.makedirs(cls.claude_fake_home, exist_ok=True)

        # Claude Code reads hooks from .claude/settings.json
        # Format: {"hooks": {"PostToolUse": [...]}}
        claude_settings = {
            "hooks": {
                "PostToolUse": [
                    {
                        "matcher": ".*",
                        "hooks": [
                            {
                                "type": "command",
                                "command": hook_script_path,
                                "timeout": 30
                            }
                        ]
                    }
                ]
            }
        }

        # Create .claude directory and settings.json file
        claude_config_dir = os.path.join(cls.claude_fake_home, ".claude")
        os.makedirs(claude_config_dir, exist_ok=True)

        claude_settings_file = os.path.join(claude_config_dir, "settings.json")
        with open(claude_settings_file, 'w') as f:
            json.dump(claude_settings, f, indent=2)

        # Also create a basic .claude.json file that Claude expects
        claude_dot_json = {
            "installMethod": "test",
            "autoUpdates": False,
            "firstStartTime": "2025-09-23T00:00:00.000Z",
            "userID": "test-user-123",
            "projects": {}
        }

        claude_dot_json_file = os.path.join(cls.claude_fake_home, ".claude.json")
        with open(claude_dot_json_file, 'w') as f:
            json.dump(claude_dot_json, f, indent=2)

        # Set up Codex hooks in a temporary directory as well
        cls.codex_fake_home = os.path.join(cls.test_dir, "fake_codex_home")
        codex_config_dir = os.path.join(cls.codex_fake_home, ".codex")
        os.makedirs(codex_config_dir, exist_ok=True)

        # Codex uses --rollout-hook command line option, so we don't need to create config files
        # But we should set CODEX_HOME to avoid polluting the real ~/.codex directory
        cls.codex_rollout_hook = hook_script_path

    @classmethod
    def setup_test_files(cls):
        """Create test playbooks and configuration files."""
        
        # Create comprehensive playbook for file operations
        cls.file_ops_playbook = {
            "rules": [
                {
                    "if_contains": ["create", "hello.py"],
                    "response": {
                        "assistant": "I'll create hello.py with a print statement.",
                        "tool_calls": [
                            {
                                "name": "write_file", 
                                "args": {
                                    "path": "hello.py", 
                                    "text": "print('Hello, World!')\n"
                                }
                            }
                        ]
                    }
                },
                {
                    "if_contains": ["read", "hello.py"],
                    "response": {
                        "assistant": "Reading the contents of hello.py",
                        "tool_calls": [
                            {
                                "name": "read_file",
                                "args": {"path": "hello.py"}
                            }
                        ]
                    }
                },
                {
                    "if_contains": ["modify", "hello.py", "add", "comment"],
                    "response": {
                        "assistant": "I'll add a comment to hello.py",
                        "tool_calls": [
                            {
                                "name": "write_file",
                                "args": {
                                    "path": "hello.py",
                                    "text": "# This is a simple hello world program\nprint('Hello, World!')\n"
                                }
                            }
                        ]
                    }
                },
                {
                    "if_contains": ["create", "calculator.py"],
                    "response": {
                        "assistant": "I'll create a simple calculator program.",
                        "tool_calls": [
                            {
                                "name": "write_file",
                                "args": {
                                    "path": "calculator.py",
                                    "text": "def add(a, b):\n    return a + b\n\ndef subtract(a, b):\n    return a - b\n\nif __name__ == '__main__':\n    print('Calculator ready')\n"
                                }
                            }
                        ]
                    }
                },
                {
                    "if_contains": ["test", "calculator"],
                    "response": {
                        "assistant": "I'll create a test file for the calculator.",
                        "tool_calls": [
                            {
                                "name": "write_file",
                                "args": {
                                    "path": "test_calculator.py",
                                    "text": "import unittest\nfrom calculator import add, subtract\n\nclass TestCalculator(unittest.TestCase):\n    def test_add(self):\n        self.assertEqual(add(2, 3), 5)\n    \n    def test_subtract(self):\n        self.assertEqual(subtract(5, 3), 2)\n\nif __name__ == '__main__':\n    unittest.main()\n"
                                }
                            }
                        ]
                    }
                },
                {
                    "if_contains": ["run", "test"],
                    "response": {
                        "assistant": "I'll run the tests for you.",
                        "tool_calls": [
                            {
                                "name": "run_command",
                                "args": {"command": "python test_calculator.py"}
                            }
                        ]
                    }
                }
            ]
        }
        
        cls.playbook_path = os.path.join(cls.test_dir, "integration_playbook.json")
        with open(cls.playbook_path, 'w') as f:
            json.dump(cls.file_ops_playbook, f, indent=2)
    
    @classmethod
    def start_mock_server(cls):
        """Start the mock API server in a separate thread."""
        def run_server():
            try:
                # Create a session directory for the server
                session_dir = os.path.join(cls.test_dir, "sessions")
                os.makedirs(session_dir, exist_ok=True)
                
                serve(
                    host=cls.server_host,
                    port=cls.server_port, 
                    playbook=cls.playbook_path,
                    codex_home=session_dir,
                    format="codex"
                )
            except Exception as e:
                print(f"Server error: {e}")
        
        cls.server_thread = threading.Thread(target=run_server, daemon=True)
        cls.server_thread.start()
    
    def setUp(self):
        """Set up each test with a fresh workspace."""
        self.workspace = tempfile.mkdtemp(prefix="workspace_", dir=self.test_dir)
        
    def tearDown(self):
        """Clean up after each test."""
        shutil.rmtree(self.workspace, ignore_errors=True)

        # Clean up workspace file
        workspace_file = os.path.join(os.path.dirname(__file__), "..", "MOCK_AGENT_WORKSPACE.txt")
        try:
            os.remove(workspace_file)
        except FileNotFoundError:
            pass
    
    def is_tool_available(self, tool_name: str) -> bool:
        """Check if a CLI tool is available in PATH."""
        try:
            result = subprocess.run(
                [tool_name, "--version"], 
                capture_output=True, 
                text=True, 
                timeout=10
            )
            return result.returncode == 0
        except (subprocess.TimeoutExpired, FileNotFoundError):
            return False
    
    def run_codex_command(self, prompt: str, **kwargs) -> subprocess.CompletedProcess:
        """Run a codex command with the mock server."""
        cmd = [
            "codex",
            "--rollout-hook", self.codex_rollout_hook,
            "--dangerously-bypass-approvals-and-sandbox",
            "--skip-git-repo-check",
            "--json",
            "-C", self.workspace,
            prompt
        ]

        # Set environment to use our mock server and fake home directory
        env = os.environ.copy()
        env["CODEX_API_BASE"] = f"http://{self.server_host}:{self.server_port}/v1"
        env["CODEX_API_KEY"] = "mock-key"
        env["CODEX_HOME"] = self.codex_fake_home
        
        return subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            env=env,
            timeout=30,
            **kwargs
        )

    def verify_hooks_executed(self, expected_executions: int, agent_type: str = "claude"):
        """Verify that hooks were executed by checking the hook execution log."""
        hook_execution_log = os.path.join(self.workspace, ".ah", "snapshots", "hook_executions.log")

        # Wait a moment for hooks to complete
        time.sleep(1)

        # Check that hook execution log exists
        self.assertTrue(os.path.exists(hook_execution_log),
                       f"Hook execution log should exist at {hook_execution_log}")

        # Read and parse execution log
        with open(hook_execution_log, 'r') as f:
            execution_lines = f.readlines()

        # Should have at least the expected number of executions
        self.assertGreaterEqual(len(execution_lines), expected_executions,
                               f"Expected at least {expected_executions} hook executions, got {len(execution_lines)}")

        # Parse and verify executions
        executions = []
        for line in execution_lines[-expected_executions:]:  # Check last N executions
            try:
                execution = json.loads(line.strip())
                executions.append(execution)
            except json.JSONDecodeError:
                self.fail(f"Invalid JSON in hook execution log: {line}")

        # Verify each execution has required fields
        for execution in executions:
            self.assertIn("timestamp", execution)
            self.assertIn("execution_id", execution)
            self.assertEqual(execution["agent_type"], agent_type)
            self.assertIn("exec-", execution["execution_id"])

        return executions

    def verify_snapshot_hooks_called(self, expected_snapshots: int, agent_type: str = "claude"):
        """Verify that filesystem snapshot hooks were called and created evidence."""
        evidence_file = os.path.join(self.workspace, ".ah", "snapshots", "evidence.log")

        # Wait a moment for hooks to complete
        time.sleep(1)

        # Check that evidence file exists
        self.assertTrue(os.path.exists(evidence_file),
                       f"Snapshot evidence file should exist at {evidence_file}")

        # Read and parse evidence file
        with open(evidence_file, 'r') as f:
            evidence_lines = f.readlines()

        # Should have at least the expected number of snapshots
        self.assertGreaterEqual(len(evidence_lines), expected_snapshots,
                               f"Expected at least {expected_snapshots} snapshots, got {len(evidence_lines)}")

        # Parse and verify snapshots
        snapshots = []
        for line in evidence_lines[-expected_snapshots:]:  # Check last N snapshots
            try:
                snapshot = json.loads(line.strip())
                snapshots.append(snapshot)
            except json.JSONDecodeError:
                self.fail(f"Invalid JSON in evidence file: {line}")

        # Verify each snapshot has required fields
        for snapshot in snapshots:
            self.assertIn("timestamp", snapshot)
            self.assertIn("tool_name", snapshot)
            self.assertIn("snapshot_id", snapshot)
            self.assertEqual(snapshot["provider"], "integration-test-fs-snapshot")
            self.assertEqual(snapshot["agent_type"], agent_type)

        return snapshots

    def run_claude_command(self, prompt: str, **kwargs) -> subprocess.CompletedProcess:
        """Run a claude command with the mock server."""
        cmd = [
            "claude",
            "--dangerously-skip-permissions",
            prompt
        ]

        # Write workspace to a file that the server can read
        workspace_file = os.path.join(os.path.dirname(__file__), "..", "MOCK_AGENT_WORKSPACE.txt")
        with open(workspace_file, "w") as f:
            f.write(self.workspace)

        # Set environment to use our mock server and fake home directory
        env = os.environ.copy()
        env["ANTHROPIC_BASE_URL"] = f"http://{self.server_host}:{self.server_port}"
        env["ANTHROPIC_API_KEY"] = "mock-key"
        env["HOME"] = self.claude_fake_home

        return subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            env=env,
            cwd=self.workspace,
            timeout=30,
            **kwargs
        )
    
    @unittest.skipUnless(subprocess.run(["which", "codex"], capture_output=True).returncode == 0,
                         "codex not available in PATH")
    def test_codex_file_creation(self):
        """Test that codex can create files through the mock agent and hooks are called."""
        if PEXPECT_AVAILABLE:
            self.skipTest("Interactive test available, skipping json mode test")

        result = self.run_codex_command("Create hello.py that prints Hello, World!")

        # Check that codex ran successfully
        self.assertEqual(result.returncode, 0, f"Codex failed: {result.stderr}")

        # Check that the file was created in workspace
        hello_file = os.path.join(self.workspace, "hello.py")
        self.assertTrue(os.path.exists(hello_file), "hello.py was not created")

        # Check file contents
        with open(hello_file, 'r') as f:
            content = f.read()
        self.assertIn("Hello, World!", content)

        # Verify that hooks were executed (basic execution evidence)
        self.verify_hooks_executed(expected_executions=1, agent_type="codex")

    @unittest.skipUnless(subprocess.run(["which", "codex"], capture_output=True).returncode == 0,
                         "codex not available in PATH")
    def test_codex_file_creation_interactive(self):
        """Test Codex interactive session with scenario-driven automation."""
        scenario_file = os.path.join(os.path.dirname(__file__), "..", "scenarios", "codex_file_creation.json")
        with open(scenario_file, 'r') as f:
            scenario = json.load(f)

        success = self.run_interactive_scenario("codex", scenario, record_session=True)
        self.assertTrue(success, "Codex interactive scenario failed")

    
    @unittest.skipUnless(subprocess.run(["which", "codex"], capture_output=True).returncode == 0,
                         "codex not available in PATH")
    @unittest.skip("Multi-step workflow test needs debugging - skipping for now")
    def test_codex_multi_step_workflow(self):
        """Test a multi-step workflow with codex."""
        # Step 1: Create calculator
        result1 = self.run_codex_command("Create calculator.py with add and subtract functions")
        self.assertEqual(result1.returncode, 0, f"Step 1 failed: {result1.stderr}")

        # Verify calculator was created
        calc_file = os.path.join(self.workspace, "calculator.py")
        self.assertTrue(os.path.exists(calc_file))

        # Step 2: Create tests
        result2 = self.run_codex_command("test calculator")
        self.assertEqual(result2.returncode, 0, f"Step 2 failed: {result2.stderr}")

        # Verify test file was created
        test_file = os.path.join(self.workspace, "test_calculator.py")
        if not os.path.exists(test_file):
            # Debug: List workspace contents and check what was created
            print(f"Workspace contents: {os.listdir(self.workspace)}")
            # Check if any test files were created with different names
            test_files = [f for f in os.listdir(self.workspace) if f.startswith('test') and f.endswith('.py')]
            if test_files:
                test_file = os.path.join(self.workspace, test_files[0])
                print(f"Found test file: {test_files[0]}")
            else:
                self.fail("No test file found")

        # Check that both files have expected content
        with open(calc_file, 'r') as f:
            calc_content = f.read()
        self.assertIn("def add", calc_content)
        self.assertIn("def subtract", calc_content)

        with open(test_file, 'r') as f:
            test_content = f.read()
        self.assertIn("assert", test_content)
        self.assertIn("calculator.", test_content)
    
    @unittest.skipUnless(subprocess.run(["which", "codex"], capture_output=True).returncode == 0, 
                         "codex not available in PATH")
    def test_codex_file_modification(self):
        """Test that codex can modify existing files."""
        # First create a file
        result1 = self.run_codex_command("Create hello.py that prints Hello, World!")
        self.assertEqual(result1.returncode, 0)
        
        # Then modify it
        result2 = self.run_codex_command("Modify hello.py to add a comment at the top")
        self.assertEqual(result2.returncode, 0)
        
        # Check the modified content
        hello_file = os.path.join(self.workspace, "hello.py")
        with open(hello_file, 'r') as f:
            content = f.read()
        
        self.assertIn("#", content, "Comment was not added")
        self.assertIn("Hello, World!", content, "Original content was lost")
    
    def run_interactive_scenario(self, tool_name: str, scenario: Dict[str, Any], record_session: bool = False) -> bool:
        """Run an interactive scenario with a CLI tool using pexpect.

        Args:
            tool_name: 'codex' or 'claude'
            scenario: Scenario definition with steps and expectations
            record_session: Whether to record the session with asciinema

        Returns:
            True if scenario completed successfully
        """
        if not PEXPECT_AVAILABLE:
            self.skipTest("pexpect not available for interactive testing")

        # Write workspace to a file that the server can read
        workspace_file = os.path.join(os.path.dirname(__file__), "..", "MOCK_AGENT_WORKSPACE.txt")
        with open(workspace_file, "w") as f:
            f.write(self.workspace)

        # Set up environment based on tool
        env = os.environ.copy()
        if tool_name == "claude":
            env["ANTHROPIC_BASE_URL"] = f"http://{self.server_host}:{self.server_port}"
            env["ANTHROPIC_API_KEY"] = "mock-key"
            env["HOME"] = self.claude_fake_home  # Use fake home with hook configuration
            if scenario.get("prompt"):
                cmd = ["claude", scenario["prompt"]]
            else:
                cmd = ["claude"]
        elif tool_name == "codex":
            env["CODEX_API_BASE"] = f"http://{self.server_host}:{self.server_port}/v1"
            env["CODEX_API_KEY"] = "mock-key"
            env["CODEX_HOME"] = self.codex_fake_home  # Use fake home directory
            if scenario.get("prompt"):
                cmd = ["codex", "exec", "--dangerously-bypass-approvals-and-sandbox", "--skip-git-repo-check", scenario["prompt"]]
            else:
                cmd = ["codex"]
        else:
            raise ValueError(f"Unknown tool: {tool_name}")

        # Set up asciinema recording if requested
        if record_session:
            import datetime
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            scenario_name = scenario.get("description", "interactive_session").replace(" ", "_").lower()
            recording_filename = f"{tool_name}_{scenario_name}_{timestamp}.json"
            recordings_dir = os.path.join(os.path.dirname(__file__), "..", "recordings")
            os.makedirs(recordings_dir, exist_ok=True)
            recording_file = os.path.join(recordings_dir, recording_filename)

            # Start asciinema recording in background
            print(f"Recording session to: {recording_filename}")

            # Use subprocess to start asciinema recording
            import subprocess
            import shlex
            # Properly quote the command arguments
            full_cmd = " ".join(shlex.quote(arg) for arg in cmd)
            asciinema_cmd = [
                "asciinema", "rec",
                "--overwrite",
                "--command", full_cmd,
                recording_file
            ]

            # Start the recording process
            recording_process = subprocess.Popen(
                asciinema_cmd,
                env=env,
                cwd=self.workspace,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE
            )

            # Give asciinema a moment to start
            time.sleep(2)

            # Now run the scenario without recording (since asciinema is already recording)
            record_session = False

        # Start the CLI tool directly
        child = pexpect.spawn(cmd[0], cmd[1:], env=env, timeout=10, cwd=self.workspace)

        try:
            # Execute scenario steps
            for i, step in enumerate(scenario.get("steps", [])):
                step_type = step["type"]
                desc = step.get("description", f"Step {i}")

                if step_type == "expect":
                    # Wait for expected output
                    patterns = []
                    for pattern in step["patterns"]:
                        if pattern == "TIMEOUT":
                            patterns.append(pexpect.TIMEOUT)
                        elif pattern == "EOF":
                            patterns.append(pexpect.EOF)
                        else:
                            patterns.append(pattern)

                    timeout = step.get("timeout", 10)
                    index = child.expect(patterns, timeout=timeout)
                    if "expected_index" in step:
                        self.assertEqual(index, step["expected_index"],
                                       f"Expected pattern {step['expected_index']}, got {index}")

                elif step_type == "send":
                    # Send input
                    text = step["text"]
                    delay = step.get("delay", 0.1)
                    time.sleep(delay)
                    if step.get("sendline", True):
                        child.sendline(text)
                    else:
                        child.send(text)

                elif step_type == "wait":
                    # Just wait
                    time.sleep(step.get("seconds", 1))

            # Check expected results
            for expectation in scenario.get("expectations", []):
                exp_type = expectation["type"]

                if exp_type == "file_exists":
                    filepath = os.path.join(self.workspace, expectation["path"])
                    self.assertTrue(os.path.exists(filepath),
                                  f"Expected file {expectation['path']} was not created")

                elif exp_type == "file_contains":
                    filepath = os.path.join(self.workspace, expectation["path"])
                    self.assertTrue(os.path.exists(filepath),
                                  f"Expected file {expectation['path']} does not exist")
                    with open(filepath, 'r') as f:
                        content = f.read()
                    self.assertIn(expectation["text"], content,
                                f"File {expectation['path']} doesn't contain expected text")

            return True

        finally:
            # Clean up the child process
            if child.isalive():
                try:
                    child.close(force=True)
                except:
                    child.terminate(force=True)

    @unittest.skipUnless(subprocess.run(["which", "claude"], capture_output=True).returncode == 0,
                         "claude not available in PATH")
    def test_claude_file_creation_interactive(self):
        """Test Claude Code interactive session with hook verification."""
        scenario_file = os.path.join(os.path.dirname(__file__), "..", "scenarios", "claude_file_creation.json")
        with open(scenario_file, 'r') as f:
            scenario = json.load(f)

        success = self.run_interactive_scenario("claude", scenario, record_session=True)
        self.assertTrue(success, "Claude interactive scenario failed")

        # Note: Claude hooks don't work in API client mode, even with --print
        # Hook verification would need full interactive mode without API server override

        # Create a recording showing Claude with --print mode (functional but not interactive UI)
        import datetime
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        recording_filename = f"claude_file_creation_{timestamp}.json"
        recordings_dir = os.path.join(os.path.dirname(__file__), "..", "recordings")
        os.makedirs(recordings_dir, exist_ok=True)
        recording_file = os.path.join(recordings_dir, recording_filename)

        # Create a command that runs Claude with --print (works reliably)
        demo_cmd = [
            "claude",
            "--print",
            "--dangerously-skip-permissions",
            "Create hello.py that prints Hello, World!"
        ]

        import shlex
        asciinema_cmd = [
            "asciinema", "rec",
            "--overwrite",
            "--command", " ".join(shlex.quote(arg) for arg in demo_cmd),
            recording_file
        ]

        # Set environment for the recording
        env = os.environ.copy()
        env["ANTHROPIC_BASE_URL"] = f"http://{self.server_host}:{self.server_port}"
        env["ANTHROPIC_API_KEY"] = "mock-key"

        # Run asciinema to create recording
        print(f"Recording Claude session to: {recording_filename}")
        try:
            result = subprocess.run(
                asciinema_cmd,
                capture_output=True,
                text=True,
                timeout=30,
                env=env,
                cwd=self.workspace
            )
            # Check that recording succeeded
            if result.returncode != 0:
                print(f"Warning: Recording failed: {result.stderr}")
        except subprocess.TimeoutExpired:
            print("Warning: Recording timed out")

        # Note: Recording creation is best-effort; test passes as long as side effects are verified

    @unittest.skipUnless(subprocess.run(["which", "claude"], capture_output=True).returncode == 0,
                         "claude not available in PATH")
    def test_claude_file_creation(self):
        """Test that claude can create files through the mock agent and hooks are called."""
        # Skip this if interactive test is available
        if PEXPECT_AVAILABLE:
            self.skipTest("Interactive test is available, skipping print mode test")

        result = self.run_claude_command("Create hello.py that prints Hello, World!")

        # Check that claude ran successfully
        self.assertEqual(result.returncode, 0, f"Claude failed: {result.stderr}")

        # Check that the file was created in workspace
        hello_file = os.path.join(self.workspace, "hello.py")
        self.assertTrue(os.path.exists(hello_file), "hello.py was not created")

        # Check file contents
        with open(hello_file, 'r') as f:
            content = f.read()
        self.assertIn("Hello, World!", content)

        # TODO: Claude hooks may not work in API client mode
        # Interactive tests should verify hooks work in normal UI mode
        # self.verify_hooks_executed(expected_executions=1, agent_type="claude")

    @unittest.skipUnless(subprocess.run(["which", "claude"], capture_output=True).returncode == 0,
                         "claude not available in PATH")
    def test_claude_file_modification(self):
        """Test that claude can modify existing files and hooks are called for each operation."""
        # First create a file
        result1 = self.run_claude_command("Create hello.py that prints Hello, World!")
        self.assertEqual(result1.returncode, 0, f"Initial creation failed: {result1.stderr}")

        # Then modify it
        result2 = self.run_claude_command("Modify hello.py to add a comment at the top")
        self.assertEqual(result2.returncode, 0, f"Modification failed: {result2.stderr}")

        # Check the modified content
        hello_file = os.path.join(self.workspace, "hello.py")
        with open(hello_file, 'r') as f:
            content = f.read()

        self.assertIn("#", content, "Comment was not added")
        self.assertIn("Hello, World!", content, "Original content was lost")

        # TODO: Claude hooks may not work in API client mode
        # Interactive tests should verify hooks work in normal UI mode
        # self.verify_hooks_executed(expected_executions=2, agent_type="claude")
    
    def test_server_health_check(self):
        """Basic test to verify the mock server is responding."""
        import urllib.request
        import urllib.error
        
        try:
            response = urllib.request.urlopen(f"http://{self.server_host}:{self.server_port}/v1/chat/completions")
            # Server should respond (even if it's an error due to empty POST)
            self.fail("Server should have rejected GET request")
        except urllib.error.HTTPError as e:
            # Expected - server should reject GET requests
            self.assertIn(str(e.code), ["405", "404", "501"])  # Method not allowed, not found, or not implemented
    
    def test_workspace_isolation(self):
        """Test that different test runs are properly isolated."""
        # Create a file in current workspace
        test_file = os.path.join(self.workspace, "isolation_test.txt")
        with open(test_file, 'w') as f:
            f.write("test content")
        
        self.assertTrue(os.path.exists(test_file))
        
        # Verify file exists only in this workspace
        other_workspace = tempfile.mkdtemp(prefix="other_workspace_", dir=self.test_dir)
        other_test_file = os.path.join(other_workspace, "isolation_test.txt")
        self.assertFalse(os.path.exists(other_test_file))
        
        # Clean up
        shutil.rmtree(other_workspace)

    def test_snapshot_hooks_claude(self):
        """Test that snapshot hooks are executed and evidence files are created."""
        # Create test workspace
        workspace = tempfile.mkdtemp(prefix="snapshot_test_")
        try:
            # Run scenario with hooks
            scenario_path = os.path.join(os.path.dirname(__file__), '..', 'examples', 'snapshot_test_scenario.json')

            # Run the mock agent directly (not via CLI tools) to test hooks
            from src.agent import run_scenario
            session_path = run_scenario(scenario_path, workspace, format="claude")

            # Verify session file was created
            self.assertTrue(os.path.exists(session_path))

            # Verify hello.py was created
            hello_py = os.path.join(workspace, "hello.py")
            self.assertTrue(os.path.exists(hello_py))

            # Verify snapshot evidence file exists
            evidence_file = os.path.join(workspace, ".ah", "snapshots", "evidence.log")
            self.assertTrue(os.path.exists(evidence_file), "Snapshot evidence file should exist")

            # Read and verify evidence file contents
            with open(evidence_file, 'r', encoding='utf-8') as f:
                evidence_lines = f.readlines()

            # Should have 3 snapshots (write_file, read_file, append_file)
            self.assertEqual(len(evidence_lines), 3, f"Expected 3 snapshots, got {len(evidence_lines)}")

            # Parse and verify each snapshot
            snapshots = [json.loads(line.strip()) for line in evidence_lines]

            # Verify first snapshot (write_file)
            write_snapshot = snapshots[0]
            self.assertEqual(write_snapshot['tool_name'], 'write_file')
            self.assertEqual(write_snapshot['tool_input']['path'], 'hello.py')
            self.assertTrue(write_snapshot['tool_response']['success'])
            self.assertEqual(write_snapshot['session_id'], 'test-session-snapshots-123')
            self.assertIn('snapshot-', write_snapshot['snapshot_id'])

            # Verify second snapshot (read_file)
            read_snapshot = snapshots[1]
            self.assertEqual(read_snapshot['tool_name'], 'read_file')
            self.assertEqual(read_snapshot['tool_input']['path'], 'hello.py')
            self.assertTrue(read_snapshot['tool_response']['success'])

            # Verify third snapshot (append_file)
            append_snapshot = snapshots[2]
            self.assertEqual(append_snapshot['tool_name'], 'append_file')
            self.assertEqual(append_snapshot['tool_input']['path'], 'hello.py')
            self.assertTrue(append_snapshot['tool_response']['success'])

            # Verify timestamps are in order
            timestamps = [s['timestamp'] for s in snapshots]
            self.assertEqual(timestamps, sorted(timestamps), "Snapshots should be in chronological order")

        finally:
            # Clean up
            shutil.rmtree(workspace, ignore_errors=True)

    def test_snapshot_hooks_codex(self):
        """Test that snapshot hooks work with Codex format as well."""
        # Create test workspace
        workspace = tempfile.mkdtemp(prefix="snapshot_codex_test_")
        try:
            # Run scenario with hooks using Codex format
            scenario_path = os.path.join(os.path.dirname(__file__), '..', 'examples', 'snapshot_test_scenario.json')

            # Run the mock agent directly to test hooks
            from src.agent import run_scenario
            session_path = run_scenario(scenario_path, workspace, format="codex")

            # Verify session file was created
            self.assertTrue(os.path.exists(session_path))

            # Verify snapshot evidence file exists and has correct content
            evidence_file = os.path.join(workspace, ".ah", "snapshots", "evidence.log")
            self.assertTrue(os.path.exists(evidence_file))

            with open(evidence_file, 'r', encoding='utf-8') as f:
                evidence_lines = f.readlines()

            # Should have 3 snapshots
            self.assertEqual(len(evidence_lines), 3)

        finally:
            # Clean up
            shutil.rmtree(workspace, ignore_errors=True)


def main():
    """Run the integration tests."""
    # Check if required tools are available
    tools_available = []
    for tool in ["codex", "claude"]:
        if subprocess.run(["which", tool], capture_output=True).returncode == 0:
            tools_available.append(tool)
    
    if not tools_available:
        print("WARNING: Neither codex nor claude CLI tools are available in PATH")
        print("Some tests will be skipped.")
    else:
        print(f"Available CLI tools: {', '.join(tools_available)}")
    
    # Run the tests
    unittest.main(verbosity=2)


if __name__ == "__main__":
    main()